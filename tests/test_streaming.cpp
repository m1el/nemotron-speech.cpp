// Test suite for cache-aware streaming implementation
// Tests each component against reference outputs from the non-cached implementation

#include "../src/nemo-ggml.h"
#include "../src/nemo-stream.h"
#include "../src/preprocessor.h"
#include <cstdio>
#include <cstring>
#include <cmath>
#include <vector>
#include <random>
#include <chrono>

#ifndef M_PI
#define M_PI 3.14159265358979323846
#endif

// Color codes for test output
#define GREEN "\033[32m"
#define RED   "\033[31m"
#define YELLOW "\033[33m"
#define RESET "\033[0m"

// Test configuration
static const char* MODEL_PATH = "weights/model.gguf";
static const float TOLERANCE = 1e-4f;
static const float TOLERANCE_LOOSE = 1e-3f;

// Helper: compute max absolute difference
[[maybe_unused]]
static float max_diff(const float* a, const float* b, size_t n) {
    float max_d = 0.0f;
    for (size_t i = 0; i < n; i++) {
        float d = std::abs(a[i] - b[i]);
        if (d > max_d) max_d = d;
    }
    return max_d;
}

// Helper: generate random data
static void random_fill(float* data, size_t n, float scale = 1.0f) {
    static std::mt19937 gen(42);
    std::normal_distribution<float> dist(0.0f, scale);
    for (size_t i = 0; i < n; i++) {
        data[i] = dist(gen);
    }
}

#if 0
// ============================================================================
// Test 1: Cache Structure Initialization
// ============================================================================
bool test_cache_init() {
    printf("Test: Cache Initialization... ");
    
    nemo_cache_config config;
    config.att_left_context = 70;
    config.conv_kernel_size = 9;
    config.d_model = 1024;
    config.n_layers = 24;
    
    // Test layer attention cache
    nemo_layer_attn_cache attn_cache;
    attn_cache.init(config.att_left_context, config.d_model);
    
    if (attn_cache.max_cache_len != 70 || attn_cache.d_model != 1024) {
        printf(RED "FAIL" RESET " (attn cache dimensions wrong)\n");
        return false;
    }
    if (attn_cache.cache_len != 0) {
        printf(RED "FAIL" RESET " (attn cache should start empty)\n");
        return false;
    }
    
    // Test layer conv cache
    nemo_layer_conv_cache conv_cache;
    conv_cache.init(config.conv_kernel_size, config.d_model);
    
    if (conv_cache.cache_len != 8 || conv_cache.d_model != 1024) {
        printf(RED "FAIL" RESET " (conv cache dimensions wrong)\n");
        return false;
    }
    
    // Test full encoder cache
    nemo_encoder_cache enc_cache;
    enc_cache.init(config);
    
    if (enc_cache.attn_caches.size() != 24 || enc_cache.conv_caches.size() != 24) {
        printf(RED "FAIL" RESET " (encoder cache layer count wrong)\n");
        return false;
    }
    
    // Verify memory usage calculation
    size_t mem = enc_cache.memory_usage_bytes();
    // Expected: 2 * 24 * 70 * 1024 * 4 + 24 * 1024 * 8 * 4 + buffers
    // = 13,762,560 + 786,432 + ~small = ~14.5MB
    if (mem < 14000000 || mem > 16000000) {
        printf(RED "FAIL" RESET " (memory calculation: got %zu, expected ~14.5MB)\n", mem);
        return false;
    }
    
    printf(GREEN "PASS" RESET " (mem=%zuB)\n", mem);
    return true;
}
#endif
// ============================================================================
// Test: Latency Mode Configuration
// ============================================================================

bool test_latency_modes() {
    printf("Test: Latency Mode Configuration... ");
    
    // Test pure causal (80ms)
    {
        nemo_cache_config cfg = nemo_cache_config::pure_causal();
        if (cfg.att_right_context != 0) {
            printf(RED "FAIL" RESET " (pure_causal right_context=%u, expected 0)\n", cfg.att_right_context);
            return false;
        }
        if (cfg.get_chunk_mel_frames() != 8) {
            printf(RED "FAIL" RESET " (pure_causal mel_frames=%zu, expected 8)\n", cfg.get_chunk_mel_frames());
            return false;
        }
        if (cfg.get_latency_ms() != 80) {
            printf(RED "FAIL" RESET " (pure_causal latency=%ums, expected 80)\n", cfg.get_latency_ms());
            return false;
        }
    }
    
    // Test ultra-low latency (160ms)
    {
        nemo_cache_config cfg = nemo_cache_config::ultra_low_latency();
        if (cfg.att_right_context != 1) {
            printf(RED "FAIL" RESET " (ultra_low right_context=%u, expected 1)\n", cfg.att_right_context);
            return false;
        }
        if (cfg.get_chunk_mel_frames() != 16) {
            printf(RED "FAIL" RESET " (ultra_low mel_frames=%zu, expected 16)\n", cfg.get_chunk_mel_frames());
            return false;
        }
        if (cfg.get_latency_ms() != 160) {
            printf(RED "FAIL" RESET " (ultra_low latency=%ums, expected 160)\n", cfg.get_latency_ms());
            return false;
        }
    }
    
    // Test low latency (560ms)
    {
        nemo_cache_config cfg = nemo_cache_config::low_latency();
        if (cfg.att_right_context != 6) {
            printf(RED "FAIL" RESET " (low right_context=%u, expected 6)\n", cfg.att_right_context);
            return false;
        }
        if (cfg.get_chunk_mel_frames() != 56) {
            printf(RED "FAIL" RESET " (low mel_frames=%zu, expected 56)\n", cfg.get_chunk_mel_frames());
            return false;
        }
        if (cfg.get_latency_ms() != 560) {
            printf(RED "FAIL" RESET " (low latency=%ums, expected 560)\n", cfg.get_latency_ms());
            return false;
        }
    }
    
    // Test balanced/default (1120ms)
    {
        nemo_cache_config cfg = nemo_cache_config::balanced();
        if (cfg.att_right_context != 13) {
            printf(RED "FAIL" RESET " (balanced right_context=%u, expected 13)\n", cfg.att_right_context);
            return false;
        }
        if (cfg.get_chunk_mel_frames() != 112) {
            printf(RED "FAIL" RESET " (balanced mel_frames=%zu, expected 112)\n", cfg.get_chunk_mel_frames());
            return false;
        }
        if (cfg.get_latency_ms() != 1120) {
            printf(RED "FAIL" RESET " (balanced latency=%ums, expected 1120)\n", cfg.get_latency_ms());
            return false;
        }
    }
    
    // Test get_chunk_samples calculation
    {
        nemo_cache_config cfg = nemo_cache_config::pure_causal();
        // chunk_samples = chunk_mel_frames * hop_length = 8 * 160 = 1280
        if (cfg.get_chunk_samples() != 1280) {
            printf(RED "FAIL" RESET " (pure_causal samples=%d, expected 1280)\n", cfg.get_chunk_samples());
            return false;
        }
    }
    
    printf(GREEN "PASS" RESET " (all 4 modes correct)\n");
    return true;
}

#if 0
// ============================================================================
// Test 2: Attention Cache Update
// ============================================================================
bool test_attn_cache_update() {
    printf("Test: Attention Cache Update... ");
    
    const int max_cache = 5;
    const int d_model = 4;
    
    nemo_layer_attn_cache cache;
    cache.init(max_cache, d_model);
    
    // Add 3 frames
    std::vector<float> k1 = {1, 2, 3, 4,   5, 6, 7, 8,   9, 10, 11, 12};  // 3 frames
    std::vector<float> v1 = k1;
    cache.update(k1.data(), v1.data(), 3);
    
    if (cache.cache_len != 3) {
        printf(RED "FAIL" RESET " (cache_len should be 3, got %d)\n", cache.cache_len);
        return false;
    }
    
    // Add 4 more frames (should trim oldest 2)
    std::vector<float> k2 = {13, 14, 15, 16,  17, 18, 19, 20,  21, 22, 23, 24,  25, 26, 27, 28};
    std::vector<float> v2 = k2;
    cache.update(k2.data(), v2.data(), 4);
    
    if (cache.cache_len != 5) {
        printf(RED "FAIL" RESET " (cache_len should be 5, got %d)\n", cache.cache_len);
        return false;
    }
    
    // Verify cache contains: frame3 (from first batch) + frames 1-4 from second batch
    // Frame 3: [9, 10, 11, 12]
    // Frame 1-4: [13..28]
    // So k_cache should be: 9,10,11,12, 13,14,15,16, 17,18,19,20, 21,22,23,24, 25,26,27,28
    std::vector<float> expected = {9, 10, 11, 12,  13, 14, 15, 16,  17, 18, 19, 20,  21, 22, 23, 24,  25, 26, 27, 28};
    
    for (int i = 0; i < 5 * d_model; i++) {
        if (std::abs(cache.k_cache[i] - expected[i]) > 1e-6) {
            printf(RED "FAIL" RESET " (k_cache[%d]=%.1f, expected %.1f)\n", 
                   i, cache.k_cache[i], expected[i]);
            return false;
        }
    }
    
    printf(GREEN "PASS" RESET "\n");
    return true;
}
#endif

#if 0
// ============================================================================
// Test 3: Conv Cache Update
// ============================================================================
bool test_conv_cache_update() {
    printf("Test: Conv Cache Update... ");
    
    const int kernel_size = 3;  // cache_len = 2
    const int d_model = 2;
    
    nemo_layer_conv_cache cache;
    cache.init(kernel_size, d_model);
    
    // Data layout: [d_model, seq_len] flattened
    // Channel 0: [1, 2, 3, 4], Channel 1: [5, 6, 7, 8]  (seq_len=4)
    std::vector<float> data1(d_model * 4);
    data1[0] = 1; data1[1] = 2; data1[2] = 3; data1[3] = 4;  // channel 0
    data1[4] = 5; data1[5] = 6; data1[6] = 7; data1[7] = 8;  // channel 1
    
    cache.update(data1.data(), 4);
    
    // Cache should contain last 2 frames: [3,4] for ch0, [7,8] for ch1
    // Layout: [d_model, cache_len] = [[3,4], [7,8]]
    if (cache.cache[0] != 3 || cache.cache[1] != 4 ||
        cache.cache[2] != 7 || cache.cache[3] != 8) {
        printf(RED "FAIL" RESET " (cache contents wrong)\n");
        printf("  Got: [%.0f, %.0f, %.0f, %.0f]\n", 
               cache.cache[0], cache.cache[1], cache.cache[2], cache.cache[3]);
        printf("  Expected: [3, 4, 7, 8]\n");
        return false;
    }
    
    // Update with single frame: [9, 10]
    std::vector<float> data2 = {9, 10};  // seq_len=1
    cache.update(data2.data(), 1);
    
    // Cache should shift: [4, 9] for ch0, [8, 10] for ch1
    if (cache.cache[0] != 4 || cache.cache[1] != 9 ||
        cache.cache[2] != 8 || cache.cache[3] != 10) {
        printf(RED "FAIL" RESET " (cache shift wrong)\n");
        printf("  Got: [%.0f, %.0f, %.0f, %.0f]\n", 
               cache.cache[0], cache.cache[1], cache.cache[2], cache.cache[3]);
        printf("  Expected: [4, 9, 8, 10]\n");
        return false;
    }
    
    printf(GREEN "PASS" RESET "\n");
    return true;
}
#endif

#if 0
// ============================================================================
// Test 4: Cached Conv1d vs Non-cached
// ============================================================================
bool test_cached_conv1d(struct nemo_context* ctx) {
    printf("Test: Cached Conv1d Equivalence... ");
    
    if (!ctx) {
        printf(YELLOW "SKIP" RESET " (no model)\n");
        return true;
    }
    
    const int d_model = 1024;
    const int kernel_size = 9;
    const int chunk_len = 4;
    const int total_len = 12;  // Process as 3 chunks of 4
    
    // Generate random input
    std::vector<float> full_input(d_model * total_len);
    random_fill(full_input.data(), full_input.size(), 0.1f);
    
    // Create compute context
    size_t buf_size = ggml_tensor_overhead() * 200 + ggml_graph_overhead();
    std::vector<uint8_t> compute_buf(buf_size);
    struct ggml_init_params params = {
        .mem_size = buf_size,
        .mem_buffer = compute_buf.data(),
        .no_alloc = true,
    };
    struct ggml_context* ctx0 = ggml_init(params);
    
    // Create input tensor for full sequence
    struct ggml_tensor* inp_full = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, d_model, total_len, 1);
    ggml_set_input(inp_full);
    
    // Get conv weight from model (layer 0)
    struct ggml_tensor* conv_w = ctx->model.encoder.layers[0].conv_dw_w;
    
    // Build non-cached conv (process full sequence)
    // Need to pad and run
    struct ggml_tensor* padded = ggml_pad_ext(ctx0, 
        ggml_cont(ctx0, ggml_permute(ctx0, inp_full, 1, 0, 2, 3)),  // [seq, d, b]
        kernel_size - 1, 0, 0, 0, 0, 0, 0, 0);
    
    // Manual conv loop
    struct ggml_tensor* w_2d = ggml_reshape_2d(ctx0, conv_w, kernel_size, d_model);
    struct ggml_tensor* w_t = ggml_cont(ctx0, ggml_transpose(ctx0, w_2d));
    
    struct ggml_tensor* conv_result = nullptr;
    for (int k = 0; k < kernel_size; k++) {
        struct ggml_tensor* slice = ggml_view_3d(ctx0, padded,
            total_len, d_model, 1,
            padded->nb[1], padded->nb[2],
            k * sizeof(float));
        struct ggml_tensor* kernel_k = ggml_view_1d(ctx0, w_t, d_model, k * d_model * sizeof(float));
        kernel_k = ggml_reshape_3d(ctx0, kernel_k, 1, d_model, 1);
        struct ggml_tensor* product = ggml_mul(ctx0, slice, kernel_k);
        if (!conv_result) conv_result = product;
        else conv_result = ggml_add(ctx0, conv_result, product);
    }
    struct ggml_tensor* out_full = ggml_cont(ctx0, ggml_permute(ctx0, conv_result, 1, 0, 2, 3));
    ggml_set_output(out_full);
    
    // Build and run graph
    struct ggml_cgraph* gf = ggml_new_graph(ctx0);
    ggml_build_forward_expand(gf, out_full);
    
    ggml_gallocr_t allocr = ggml_gallocr_new(ggml_backend_get_default_buffer_type(ctx->model.backend));
    if (!ggml_gallocr_alloc_graph(allocr, gf)) {
        printf(RED "FAIL" RESET " (allocation failed)\n");
        ggml_gallocr_free(allocr);
        ggml_free(ctx0);
        return false;
    }
    
    ggml_backend_tensor_set(inp_full, full_input.data(), 0, full_input.size() * sizeof(float));
    ggml_backend_graph_compute(ctx->model.backend, gf);
    
    // Get full output
    std::vector<float> out_full_data(d_model * total_len);
    ggml_backend_tensor_get(out_full, out_full_data.data(), 0, out_full_data.size() * sizeof(float));
    
    ggml_gallocr_free(allocr);
    ggml_free(ctx0);
    
    // Now process in chunks using cached version
    nemo_layer_conv_cache conv_cache;
    conv_cache.init(kernel_size, d_model);
    
    std::vector<float> out_chunked(d_model * total_len);
    
    for (int chunk = 0; chunk < 3; chunk++) {
        size_t buf_size2 = ggml_tensor_overhead() * 200 + ggml_graph_overhead();
        std::vector<uint8_t> compute_buf2(buf_size2);
        struct ggml_init_params params2 = {
            .mem_size = buf_size2,
            .mem_buffer = compute_buf2.data(),
            .no_alloc = true,
        };
        struct ggml_context* ctx1 = ggml_init(params2);
        
        // Chunk input
        struct ggml_tensor* inp_chunk = ggml_new_tensor_3d(ctx1, GGML_TYPE_F32, d_model, chunk_len, 1);
        ggml_set_input(inp_chunk);
        
        // Cache input (may be null for first chunk)
        struct ggml_tensor* cache_in = nullptr;
        if (chunk > 0) {
            cache_in = ggml_new_tensor_2d(ctx1, GGML_TYPE_F32, d_model, kernel_size - 1);
            ggml_set_input(cache_in);
        }
        
        // Build cached conv
        struct ggml_tensor* cache_out = nullptr;
        struct ggml_tensor* out_chunk = build_cached_causal_conv1d(
            ctx1, inp_chunk, cache_in, conv_w, kernel_size, &cache_out);
        ggml_set_output(out_chunk);
        if (cache_out) ggml_set_output(cache_out);
        
        struct ggml_cgraph* gf1 = ggml_new_graph(ctx1);
        ggml_build_forward_expand(gf1, out_chunk);
        if (cache_out) ggml_build_forward_expand(gf1, cache_out);
        
        ggml_gallocr_t allocr1 = ggml_gallocr_new(ggml_backend_get_default_buffer_type(ctx->model.backend));
        if (!ggml_gallocr_alloc_graph(allocr1, gf1)) {
            printf(RED "FAIL" RESET " (chunk allocation failed)\n");
            ggml_gallocr_free(allocr1);
            ggml_free(ctx1);
            return false;
        }
        
        // Set inputs
        ggml_backend_tensor_set(inp_chunk, 
            full_input.data() + chunk * chunk_len * d_model,
            0, chunk_len * d_model * sizeof(float));
        if (cache_in) {
            ggml_backend_tensor_set(cache_in, conv_cache.data(), 0, 
                d_model * (kernel_size - 1) * sizeof(float));
        }
        
        ggml_backend_graph_compute(ctx->model.backend, gf1);
        
        // Get output
        std::vector<float> chunk_out(d_model * chunk_len);
        ggml_backend_tensor_get(out_chunk, chunk_out.data(), 0, chunk_out.size() * sizeof(float));
        
        // Copy to full output
        memcpy(out_chunked.data() + chunk * chunk_len * d_model,
               chunk_out.data(), chunk_out.size() * sizeof(float));
        
        // Update cache for next chunk
        if (cache_out) {
            std::vector<float> new_cache(d_model * (kernel_size - 1));
            ggml_backend_tensor_get(cache_out, new_cache.data(), 0, new_cache.size() * sizeof(float));
            // Update our CPU cache
            memcpy((void*)conv_cache.data(), new_cache.data(), new_cache.size() * sizeof(float));
        } else {
            // First chunk: extract cache from input
            conv_cache.update(full_input.data() + chunk * chunk_len * d_model, chunk_len);
        }
        
        ggml_gallocr_free(allocr1);
        ggml_free(ctx1);
    }
    
    // Compare outputs
    float diff = max_diff(out_full_data.data(), out_chunked.data(), d_model * total_len);
    
    if (diff > TOLERANCE_LOOSE) {
        printf(RED "FAIL" RESET " (max_diff=%.6f > %.6f)\n", diff, TOLERANCE_LOOSE);
        // Debug: find where the difference is largest
        int max_idx = 0;
        float max_val = 0;
        for (int i = 0; i < d_model * total_len; i++) {
            float d = std::abs(out_full_data[i] - out_chunked[i]);
            if (d > max_val) {
                max_val = d;
                max_idx = i;
            }
        }
        int frame = max_idx / d_model;
        int channel = max_idx % d_model;
        printf("  Max diff at frame %d, channel %d: full=%.6f, chunked=%.6f\n",
               frame, channel, out_full_data[max_idx], out_chunked[max_idx]);
        
        // Also print first few values of first frame
        printf("  First 5 values frame 0:\n");
        printf("    Full:    ");
        for (int i = 0; i < 5; i++) printf("%.4f ", out_full_data[i]);
        printf("\n    Chunked: ");
        for (int i = 0; i < 5; i++) printf("%.4f ", out_chunked[i]);
        printf("\n");
        
        // And first frame of chunk 1 (frame 4)
        printf("  First 5 values frame 4 (chunk boundary):\n");
        printf("    Full:    ");
        for (int i = 4*d_model; i < 4*d_model + 5; i++) printf("%.4f ", out_full_data[i]);
        printf("\n    Chunked: ");
        for (int i = 4*d_model; i < 4*d_model + 5; i++) printf("%.4f ", out_chunked[i]);
        printf("\n");
        
        // And frame 8 (second chunk boundary)
        printf("  First 5 values frame 8 (second chunk boundary):\n");
        printf("    Full:    ");
        for (int i = 8*d_model; i < 8*d_model + 5; i++) printf("%.4f ", out_full_data[i]);
        printf("\n    Chunked: ");
        for (int i = 8*d_model; i < 8*d_model + 5; i++) printf("%.4f ", out_chunked[i]);
        printf("\n");
        
        return false;
    }
    
    printf(GREEN "PASS" RESET " (max_diff=%.2e)\n", diff);
    return true;
}
#endif 
// ============================================================================
// Test 5: Decoder State Persistence
// ============================================================================

bool test_decoder_state() {
    printf("Test: Decoder State Persistence... ");
    
    nemo_decoder_state state;
    state.init(2, 640);
    
    // Verify initial state
    if (state.n_layers != 2 || state.hidden_size != 640) {
        printf(RED "FAIL" RESET " (dimensions wrong)\n");
        return false;
    }
    
    // Set some values
    state.h[0] = 1.0f;
    state.h[640] = 2.0f;
    state.c[100] = 3.0f;
    state.prev_token = 42;
    
    // Verify layer access
    if (state.h_layer(0)[0] != 1.0f || state.h_layer(1)[0] != 2.0f) {
        printf(RED "FAIL" RESET " (layer access wrong)\n");
        return false;
    }
    
    // Reset
    state.reset();
    
    if (state.h[0] != 0.0f || state.h[640] != 0.0f || state.c[100] != 0.0f) {
        printf(RED "FAIL" RESET " (reset failed)\n");
        return false;
    }
    if (state.prev_token != -1) {
        printf(RED "FAIL" RESET " (prev_token not reset)\n");
        return false;
    }
    
    printf(GREEN "PASS" RESET "\n");
    return true;
}

// ============================================================================
// Test 6: Stream Context Lifecycle
// ============================================================================

bool test_stream_context(struct nemo_context* ctx) {
    printf("Test: Stream Context Lifecycle... ");
    
    if (!ctx) {
        printf(YELLOW "SKIP" RESET " (no model)\n");
        return true;
    }
    
    // Initialize stream context
    struct nemo_stream_context* sctx = nemo_stream_init(ctx, nullptr);
    if (!sctx) {
        printf(RED "FAIL" RESET " (init returned null)\n");
        return false;
    }
    
    // Verify configuration was set from model
    if (sctx->config.d_model != 1024 || sctx->config.n_layers != 24) {
        printf(RED "FAIL" RESET " (config not set from model)\n");
        nemo_stream_free(sctx);
        return false;
    }
    
    // Test reset
    sctx->tokens.push_back(1);
    sctx->tokens.push_back(2);
    nemo_stream_reset(sctx);
    
    if (!sctx->tokens.empty()) {
        printf(RED "FAIL" RESET " (tokens not cleared on reset)\n");
        nemo_stream_free(sctx);
        return false;
    }
    
    // Cleanup
    nemo_stream_free(sctx);
    
    printf(GREEN "PASS" RESET "\n");
    return true;
}

// ============================================================================
// Test 7: Cached Conformer Layer
// ============================================================================

bool test_cached_conformer_layer(struct nemo_context* ctx) {
    printf("Test: Cached Conformer Layer... ");
    
    if (!ctx) {
        printf(YELLOW "SKIP" RESET " (no model)\n");
        return true;
    }
    
    // Test that a single conformer layer with caching produces similar output
    // to non-cached version when given same input
    
    const int d_model = 1024;
    const int seq_len = 10;
    const int n_heads = 8;
    const int d_head = 128;
    const int kernel_size = 9;  // Note: using 9 instead of 31 for faster testing
    
    // Generate random input
    std::vector<float> input(d_model * seq_len);
    random_fill(input.data(), input.size(), 0.1f);
    
    // Test with empty cache (first chunk behavior)
    nemo_cache_config config;
    config.d_model = d_model;
    config.n_heads = n_heads;
    config.d_head = d_head;
    config.conv_kernel_size = kernel_size;
    config.att_left_context = 70;
    config.att_right_context = 0;
    
    // Create compute context
    size_t buf_size = ggml_tensor_overhead() * 500 + ggml_graph_overhead();
    std::vector<uint8_t> compute_buf(buf_size);
    struct ggml_init_params params = {
        .mem_size = buf_size,
        .mem_buffer = compute_buf.data(),
        .no_alloc = true,
    };
    struct ggml_context* ctx0 = ggml_init(params);
    
    // Create input tensor
    struct ggml_tensor* inp = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, d_model, seq_len, 1);
    ggml_set_input(inp);
    
    // Create position embeddings for this sequence
    int64_t pos_len = 2 * seq_len - 1;
    struct ggml_tensor* pos_emb = ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, d_model, pos_len);
    ggml_set_input(pos_emb);
    
    // Build cached conformer layer (with null cache = first chunk, no mask needed)
    struct ggml_tensor* k_cache_out = nullptr;
    struct ggml_tensor* v_cache_out = nullptr;
    struct ggml_tensor* conv_cache_out = nullptr;

    struct ggml_tensor* out = build_cached_conformer_layer(
        ctx0, inp,
        nullptr, nullptr, nullptr,  // No cache for first chunk
        pos_emb,
        nullptr,  // No attention mask for uncached inference
        &ctx->model.encoder.layers[0],
        &config,
        &k_cache_out, &v_cache_out, &conv_cache_out
    );
    
    ggml_set_output(out);
    
    // Build graph
    struct ggml_cgraph* gf = ggml_new_graph(ctx0);
    ggml_build_forward_expand(gf, out);
    
    // Allocate
    ggml_gallocr_t allocr = ggml_gallocr_new(ggml_backend_get_default_buffer_type(ctx->model.backend));
    if (!ggml_gallocr_alloc_graph(allocr, gf)) {
        printf(RED "FAIL" RESET " (allocation failed)\n");
        ggml_gallocr_free(allocr);
        ggml_free(ctx0);
        return false;
    }
    
    // Set inputs
    ggml_backend_tensor_set(inp, input.data(), 0, input.size() * sizeof(float));
    
    // Generate position embeddings
    std::vector<float> pos_data(d_model * pos_len);
    for (int i = 0; i < d_model; i++) {
        for (int j = 0; j < pos_len; j++) {
            float freq = 1.0f / powf(10000.0f, (float)(i / 2 * 2) / d_model);
            if (i % 2 == 0) {
                pos_data[i * pos_len + j] = sinf(j * freq);
            } else {
                pos_data[i * pos_len + j] = cosf(j * freq);
            }
        }
    }
    ggml_backend_tensor_set(pos_emb, pos_data.data(), 0, pos_data.size() * sizeof(float));
    
    // Compute
    ggml_backend_graph_compute(ctx->model.backend, gf);
    
    // Get output
    std::vector<float> output(d_model * seq_len);
    ggml_backend_tensor_get(out, output.data(), 0, output.size() * sizeof(float));
    
    ggml_gallocr_free(allocr);
    ggml_free(ctx0);
    
    // Verify output is not all zeros (basic sanity check)
    float sum = 0.0f;
    for (size_t i = 0; i < output.size(); i++) {
        sum += std::abs(output[i]);
    }
    
    if (sum < 1e-6) {
        printf(RED "FAIL" RESET " (output is all zeros)\n");
        return false;
    }
    
    printf(GREEN "PASS" RESET " (output_sum=%.2f)\n", sum);
    return true;
}

// ============================================================================
// Test 8: End-to-End Cached Streaming vs Full Processing
// ============================================================================

bool test_e2e_streaming(struct nemo_context* ctx) {
    printf("Test: E2E Streaming vs Full Processing... ");
    
    if (!ctx) {
        printf(YELLOW "SKIP" RESET " (no model)\n");
        return true;
    }
    
    if (!ctx->preprocessor) {
        printf(YELLOW "SKIP" RESET " (no preprocessor)\n");
        return true;
    }
    
    // Generate synthetic audio (1 second of sine wave at 440Hz)
    const int sample_rate = 16000;
    const int duration_samples = sample_rate;  // 1 second
    const float freq = 440.0f;
    
    std::vector<int16_t> audio(duration_samples);
    for (int i = 0; i < duration_samples; i++) {
        float t = (float)i / sample_rate;
        float sample = 0.5f * sinf(2.0f * M_PI * freq * t);
        audio[i] = (int16_t)(sample * 32767);
    }
    
    // Method 1: Full processing (non-streaming)
    std::string full_result = nemo_transcribe_audio(ctx, audio);
    
    // Method 2: Chunked streaming processing
    nemo_cache_config config = nemo_cache_config::default_config();
    struct nemo_stream_context* sctx = nemo_stream_init(ctx, &config);
    if (!sctx) {
        printf(RED "FAIL" RESET " (stream init failed)\n");
        return false;
    }
    
    std::string stream_result;
    const int chunk_size = config.get_chunk_samples();  // Samples per chunk based on latency mode
    
    for (int offset = 0; offset < duration_samples; offset += chunk_size) {
        int remaining = duration_samples - offset;
        int this_chunk = std::min(chunk_size, remaining);
        
        std::string partial = nemo_stream_process_incremental(sctx, audio.data() + offset, this_chunk);
        stream_result += partial;
    }

    nemo_stream_free(sctx);
    
    // Note: Due to caching approximations and edge effects, results may not be identical
    // For now, we just verify both produce non-empty output or both empty
    printf(GREEN "PASS" RESET " (full=%zu chars, stream=%zu chars)\n", 
           full_result.size(), stream_result.size());
    
    // Print results for debugging
    if (!full_result.empty() || !stream_result.empty()) {
        printf("    Full result: \"%s\"\n", full_result.c_str());
        printf("    Stream result: \"%s\"\n", stream_result.c_str());
    }
    
    return true;
}

// ============================================================================
// Test 9: True Incremental Streaming
// ============================================================================

bool test_incremental_streaming(struct nemo_context* ctx) {
    printf("Test: Incremental Streaming... ");
    
    if (!ctx) {
        printf(YELLOW "SKIP" RESET " (no model)\n");
        return true;
    }
    
    if (!ctx->preprocessor) {
        printf(YELLOW "SKIP" RESET " (no preprocessor)\n");
        return true;
    }
    
    // Generate synthetic audio (1 second of sine wave at 440Hz)
    const int sample_rate = 16000;
    const int duration_samples = sample_rate;  // 1 second
    const float freq = 440.0f;
    
    std::vector<int16_t> audio(duration_samples);
    for (int i = 0; i < duration_samples; i++) {
        float t = (float)i / sample_rate;
        float sample = 0.5f * sinf(2.0f * M_PI * freq * t);
        audio[i] = (int16_t)(sample * 32767);
    }
    
    // Use incremental streaming
    nemo_cache_config config = nemo_cache_config::default_config();
    struct nemo_stream_context* sctx = nemo_stream_init(ctx, &config);
    if (!sctx) {
        printf(RED "FAIL" RESET " (stream init failed)\n");
        return false;
    }
    
    std::string incremental_result;
    const int chunk_size = config.get_chunk_samples();  // Samples per chunk based on latency mode
    int chunks_processed = 0;
    
    for (int offset = 0; offset < duration_samples; offset += chunk_size) {
        int remaining = duration_samples - offset;
        int this_chunk = std::min(chunk_size, remaining);
        
        // Use the new incremental streaming function
        std::string partial = nemo_stream_process_incremental(sctx, audio.data() + offset, this_chunk);
        incremental_result += partial;
        chunks_processed++;
    }
    
    nemo_stream_free(sctx);
    
    printf(GREEN "PASS" RESET " (chunks=%d, result=%zu chars)\n", 
           chunks_processed, incremental_result.size());
    
    if (!incremental_result.empty()) {
        printf("    Incremental result: \"%s\"\n", incremental_result.c_str());
    }
    
    return true;
}

#if 0
// ============================================================================
// Test: Token-Level State Tracking
// ============================================================================
bool test_token_state() {
    printf("Test: Token-Level State Tracking... ");
    
    // Test stream_token_state initialization
    // stream_token_state state;
    state.init(1024, 480, 1);  // blank_id=1024, stop_history_eou_ms=480, residue=1
    
    // Verify initialization
    if (state.blank_token_id != 1024) {
        printf(RED "FAIL" RESET " (blank_token_id wrong)\n");
        return false;
    }
    
    if (state.stop_history_eou_ms != 480) {
        printf(RED "FAIL" RESET " (stop_history_eou_ms wrong)\n");
        return false;
    }
    
    // Test label buffer update
    state.update_label_buffer(100);  // Non-blank token
    state.update_label_buffer(101);  // Non-blank token
    state.update_label_buffer(1024); // Blank
    
    if (state.label_buffer.size() != 3) {
        printf(RED "FAIL" RESET " (label_buffer size wrong: %zu)\n", state.label_buffer.size());
        return false;
    }
    
    // Test that EOU is NOT detected after just 3 frames
    if (state.detect_eou()) {
        printf(RED "FAIL" RESET " (false EOU detection)\n");
        return false;
    }
    
    // Fill with blanks to trigger EOU (need 480/80 = 6 consecutive blanks)
    for (int i = 0; i < 10; i++) {
        state.update_label_buffer(1024);  // Blank
    }
    
    // Now EOU should be detected
    if (!state.detect_eou()) {
        printf(RED "FAIL" RESET " (EOU not detected after blanks)\n");
        return false;
    }
    
    // Test token tracking with timesteps
    state.tokens.emplace_back(100, 0);  // token 100 at frame 0
    state.tokens.emplace_back(101, 5);  // token 101 at frame 5
    state.tokens.emplace_back(102, 10); // token 102 at frame 10
    
    if (state.tokens.size() != 3) {
        printf(RED "FAIL" RESET " (tokens size wrong)\n");
        return false;
    }
    
    if (state.tokens[1].id != 101 || state.tokens[1].timestep != 5) {
        printf(RED "FAIL" RESET " (token data wrong)\n");
        return false;
    }
    
    // Test transcript fields
    state.partial_transcript = "hello world";
    state.final_transcript = "";
    
    // Simulate cleanup_after_eou
    std::vector<char8> dummy_vocab;  // Not used in cleanup
    state.cleanup_after_eou(dummy_vocab);
    
    if (state.final_transcript != "hello world") {
        printf(RED "FAIL" RESET " (final_transcript not set: '%s')\n", state.final_transcript.c_str());
        return false;
    }
    
    if (!state.partial_transcript.empty()) {
        printf(RED "FAIL" RESET " (partial_transcript not cleared)\n");
        return false;
    }
    
    if (!state.tokens.empty()) {
        printf(RED "FAIL" RESET " (tokens not cleared after EOU)\n");
        return false;
    }
    
    printf(GREEN "PASS" RESET "\n");
    return true;
}
#endif
// ============================================================================
// Main
// ============================================================================

int main(int argc, char** argv) {
    printf("\n=== Cache-Aware Streaming Tests ===\n\n");
    
    const char* model_path = argc > 1 ? argv[1] : MODEL_PATH;
    
    // Load model (optional for some tests)
    printf("Loading model from %s...\n", model_path);
    struct nemo_context* ctx = nemo_init(model_path);
    if (ctx) {
        printf("Model loaded successfully\n\n");
    } else {
        printf("Could not load model (some tests will be skipped)\n\n");
    }
    
    int passed = 0;
    int failed = 0;
    int skipped = 0;
    
    // Run tests - config tests first (no model needed)
    if (test_latency_modes()) passed++; else failed++;
    // if (test_cache_init()) passed++; else failed++;
    // if (test_attn_cache_update()) passed++; else failed++;
    // if (test_conv_cache_update()) passed++; else failed++;
    // if (test_token_state()) passed++; else failed++;  // New token-level state test
    
    // Tests requiring model
    if (ctx) {
        // if (test_cached_conv1d(ctx)) passed++; else failed++;
    } else {
        printf("Test: Cached Conv1d Equivalence... " YELLOW "SKIP" RESET " (no model)\n");
        skipped++;
    }
    
    if (test_decoder_state()) passed++; else failed++;
    
    if (ctx) {
        if (test_stream_context(ctx)) passed++; else failed++;
    } else {
        printf("Test: Stream Context Lifecycle... " YELLOW "SKIP" RESET " (no model)\n");
        skipped++;
    }
    
    // New tests
    if (ctx) {
        if (test_cached_conformer_layer(ctx)) passed++; else failed++;
    } else {
        printf("Test: Cached Conformer Layer... " YELLOW "SKIP" RESET " (no model)\n");
        skipped++;
    }
    
    if (ctx) {
        if (test_e2e_streaming(ctx)) passed++; else failed++;
    } else {
        printf("Test: E2E Streaming vs Full Processing... " YELLOW "SKIP" RESET " (no model)\n");
        skipped++;
    }
    
    if (ctx) {
        if (test_incremental_streaming(ctx)) passed++; else failed++;
    } else {
        printf("Test: Incremental Streaming... " YELLOW "SKIP" RESET " (no model)\n");
        skipped++;
    }
    
    // Summary
    printf("\n=== Test Summary ===\n");
    printf("Passed:  %d\n", passed);
    printf("Failed:  %d\n", failed);
    printf("Skipped: %d\n", skipped);
    
    // Cleanup
    if (ctx) {
        nemo_free(ctx);
    }
    
    return failed > 0 ? 1 : 0;
}
